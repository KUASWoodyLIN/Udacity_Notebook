{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATH Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.path.expanduser('~')\n",
    "DATA_PATH = os.path.join(ROOT_PATH, 'dataset')\n",
    "MNIST_PATH = os.path.join(DATA_PATH, 'mnist')\n",
    "PREJECT_PATH = os.getcwd()\n",
    "MODELS_PATH = os.path.join(PREJECT_PATH, 'models/mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/woodylin/dataset/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting /home/woodylin/dataset/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting /home/woodylin/dataset/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting /home/woodylin/dataset/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(MNIST_PATH)\n",
    "# Load training and eval data\n",
    "train_data = data.train.images # Returns np.array\n",
    "train_labels = data.train.labels\n",
    "valid_data = data.validation.images # Returns np.array\n",
    "valid_labels = data.validation.labels\n",
    "test_data = data.test.images\n",
    "test_labels = data.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t55000\n",
      "- Test-set:\t\t10000\n",
      "- Validation-set:\t5000\n",
      "- Input data shape\t(55000, 784)\n",
      "- Output data shape\t(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(data.train.labels)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(data.test.labels)))\n",
    "print(\"- Validation-set:\\t{}\".format(len(data.validation.labels)))\n",
    "print(\"- Input data shape\\t{}\".format(data.train.images.shape))\n",
    "print(\"- Output data shape\\t{}\".format(data.test.labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size     = 784\n",
    "img_shape    = (28, 28)\n",
    "num_channels = 1\n",
    "num_classes  = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function for plotting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAD5CAYAAAC9FVegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHy5JREFUeJzt3XmYFMX9x/H314NTlEsxKrIxGI6gIhIBbwWiiCGAimCM/NR4PEExYoxBBA3GI6iIhkTEaCByCIgnJiCgIniAGDwSxICwIgQ55BJUDqnfH7vVPbM77O7sTk/PLp/X8/BsT3XP9Hctp/bb1dVV5pxDRGRft1/cAYiI5AI1hiIiqDEUEQHUGIqIAGoMRUQANYYiIoAaQxERQI2hiAigxlBEBIAD0jm4YcOGLi8vL6JQck9+fj4bNmywuOPIJtVx1ac6Ti2txjAvL4+FCxeWP6pKpm3btnGHkHWq46pPdZyaLpNFRFBjKCICqDEUEQHUGIqIAGoMRUQANYYiIoAaQxERQI2hiAigxlBEBEjzCRSRTPvss88AePzxx4Oyu+++GwCzgieo/KJlLVq0AOAPf/hDcGzPnj2zEqdUfcoMRURQZihZtn79egDuvfdeAMaPHw/Ahg0bgmN8Ruh/ep988gkAN998c1B2xhlnANCwYcOIIpay2rlzJwAdO3YEYN68eUn769atG2x/+OGHADRu3DhL0ZVOmaGICDmWGf7tb38DwoygQYMGAHz88cfBMR06dADg9NNPz3J0UhG+n2/w4MFA8f7AxCzw6KOPBuDQQw9N+gyfPebn5wdlPjNcvHhxBFFLWfiM8KqrrgKKZ4Tdu3cH4He/+11QdsQRR5T589euXQtAo0aNKhRnaZQZioiQwcxwwoQJACxatAiAJ598Mu3P2Lx5c9LrAw4oCM//5QGoUaMGALVq1QLg+OOPB2Dy5MlA8WxCcsMLL7wA7L0/sGXLlsH266+/DhTvB5w7dy4AZ555ZlDm+xElPg8++CAA48aNSyrv168fAA888AAQfnfLIrFf2F8xDhkyBIBf//rX5Q+2BMoMRURQYygiAlTwMnnAgAHB9sMPPwzAnj17KhZRgsTLY+/bb79N+ukvqS655BIAJk6cGBwbdYerlM7f/FqyZAlQ/OaIvxQePnx48J7bb78dgNtuuy3pPf6mmb/pkmj06NEAXHPNNZn9BSSlf//738H2XXfdlbSvTp06AIwYMQIIu7vK4t133wVgzJgxQdmmTZvKG2ZalBmKiFDBzHDKlCnBts8I/Q2NmjVrlvr+U089FQhvvZfFrFmzAPj73/8OhMMsXnvtNQD69OkTHDtp0iRAN1Xi5B+h83/xfSZY9OaIz+wSt32W5zPDZ599Fki++eK39Vhedt13333B9jfffAPAgQceCMCLL74IpJcRev5my8aNG4OyatWqAem1E+WhzFBEhApmhrNnzw62fR9C586dgbDfINN8v1Hfvn0B6Nq1KxD2SfkMEcLsMfE2vcSjefPmJe5PzBSbNWsGhIPuH3roISDMRhL7DIv2PUp2vPfee8XKzjvvPADOOuuspPLvvvsOSH0PwPv0008BmDNnTrF9F154IVCwxGmUlBmKiFDBzPCHP/xhyu1sOOaYY4DwTtbFF19c7BifSSgzzB1vvPEGEGbyPqPzfYsQDqRu164dAOvWrQPC/sHDDjssOPaf//xnxBFLWe3YsSPp9YIFC4BwdMDMmTPL/FmHH354sO1HFURNmaGICDk2UYNUff6xTX/HONVEDb7MZ4T+te8fvOGGG4Jj27RpE3HEksqtt94abF9xxRVA2F9/zjnnAGH/X3nGHl999dXBdqtWrcodZzqUGYqIUIkzw7/85S8ALFy4cK/H+PFP/s7XSSedFH1gUiZFJ2oo+jqxzE/T5Z9SUTYYv5UrVxYr27VrF5A8ogOgffv2APTo0SMoW716NQCPPPJIys9v27ZtRuJMhzJDERHUGIqIADl2mbxmzRognBfND7Yt6diSbN++HQg7dLds2VLREKWCLr30UiBcFc/PXu2H2gBs27Yt6T1Dhw4FdHmcS6688spg2z8uV1Tv3r2BcJ2T/fffP9jn18Ap6rTTTgPg/PPPz0ic6VBmKCJCzJmhn3TB3+B47LHHAFixYkVGz5P4V0zi5W+G+J9eYmY4aNAgAJ5//nkgHDTvB1jr0bv4HXXUUcF24tomZVW7du2U5f379wfKN8lDRSkzFBEhi5nh0qVLAbjuuuuCsldffbXE9zRp0gSAevXqFdvnH8Pz6ypcf/31QOo1MdJZiUvS59dCrshUaYkTOUydOhWALl26ADB9+nQg7EuOag0MyZ799tsv5eumTZvGEU5BDLGdWUQkh0SeGfo7wiNHjgRg+fLlwb6DDjoIgEMOOQSAm266CQgzuVNOOQUIM8SS+M9I5KcRu+CCC8oVu5TMT7rg+/R8dvfUU09l5PP9A/ozZswAtBJeVZI4mS/AT37yEwBOPPHEOMIBlBmKiABZyAzffvttIMwIu3XrFuzzGUXRO4vpeP/994Fw3Fqi6tWrA8nTQ0nF+T7Ca6+9FggX3spURujHh/rPT7UAlFQ+ieN8t27dmrQvF/qBlRmKiKDGUEQEyMJl8qhRo4Bw1Tw/622mLFu2DIC1a9cW29epU6eMnksKPPfcc0B4Q6Pomhfl4ddXhnDNC//5fvaa0tZRkdzmZ76GsFvLP8pXv379WGJKpMxQRIQsZIa+xc90Ruj5GzRe3bp1g23/aI9kll+h0N/Y8DMa+0HRiTesis4h6TOCuXPnAuFayP7Ru8TP9Rmh71y/8cYbM/hbSLYlzlDu+eF1P/7xj7MdTjHKDEVEyLEpvNJx3HHHAckP+EM4eBOgQ4cOWY1pX+Ezv549ewJhVnf55ZcDybNWF512y8+Q7KfuSrUGiuevJpThVw1FV88DOOGEE2KIJDVlhiIiVOLMMD8/H4Ddu3cD4eN4uTB4c1/hRwr4bC/VejS+zGd+RTPBWrVqAcn9jAMHDgTCzFOqrsQJX+OmzFBEhEqYGU6cOBGAr7/+GggnY/APfqufMHv8lF1+0tXBgwcXO8ZP2OvHDhadmNXfIdYYwn2Tn+zDL+0wZMiQ2GJRZigiQiXJDP16rADDhg0DwpHrF110EQC9evXKfmAChNneo48+WmxfqjLZNyWOM/STM2/evBkoPtlrHOKPQEQkB6gxFBGhklwmJw7I9evutm7dGoDOnTvHEpOIpGfAgAEpt3OFMkMRESpJZpi4huott9wSYyQiUlUpMxQRASyd9SXMbD1QfLGRqquJc678iwFXQqrjqk91nFpajaGISFWly2QREdQYiogAEd9NNrMGwOzCl4cD3wHrC1+f7JzbGcE5WwITEop+AAx0zo3M9LkktjquDbwGVCv897RzbmimzyMF4qjjhHMfAPwLWO6c6x7VeSCLfYZmdiewzTn3QJFyK4xjTwTnPBBYDbRxzq3K9OdLsmzVsZntB9R0zm0vrOO3geucc8UnVJSMyvb32Mx+C7QGakXdGMZymWxmTc1ssZmNB/4DNDazzQn7e5vZXwu3G5nZs2a20MwWmFn7NE7VGfhYDWH2RVnHzrk9zrnthS+rAQcCuhOYZVF/j82sCQXf4b9F9TskirPPsDnwkHOuJQXZ2948AgxzzrUFegH+P247MxtVyjl6AxMzEayUS2R1bGbVzOx9YC0wzTn3XmZDlzKK8ns8AriFLP2hi/MJlE/LeFnTCWiW8HxyPTOr6ZybD8zf25vMrAbQFci9hyD3HZHVcWE/VWszqwc8Z2YtnHMfpzpWIhVJHZtZd+Bz59z7ZtYpc+HuXZyN4faE7T1A4vJoNRK2jfJ10nYF5jvnNpQzPqm4qOsY59wmM3sDOBdQY5h9UdXxKUBPM+tW+DkHm9lY51zfCkVbgpwYWlPY6brJzI4t7BzvkbB7FtDPvzCz1mX82D7oEjlnZLKOzewwMzukcLsWBVnHkpLeI9HLZB07537rnDvKOZcHXAa8EmVDCDnSGBa6FZgBvAUk3vDoB5xqZh+a2WLgaii1P6kOcDbwfLQhS5oyVcdHAHPM7ANgAfCyc256tKFLGWXse5xtehxPRITcygxFRGKjxlBEBDWGIiKAGkMREUCNoYgIkOag64YNG7q8vLyIQsk9+fn5bNiwwUo/supQHVd9quPU0moM8/LyWLhw35kYpG3btnGHkHWq46pPdZyaLpNFRFBjKCICqDEUEQHUGIqIAPFO4VUue/YUzCp+8803AzByZMHSJm+//Tawb3aIi0jFKTMUEaGSZIbr1q0LtgcPHgzA6NGjk45ZsWIFoMywsrr66quD7XHjxgHw5ptvAtCmTZtYYpJ9izJDERFyPDNcs2YNAMOGDQvKimaEp59+OgDt2rXLXmCScU2aNAm2v/32WwCWLl0KKDOsiubNmwfAY489BoRXA6n473jPnj0BuPzyywGoX79+RmNSZigiQo5mhrt37wbg7rvvBuDPf/5zsWP69StYTmH48OEAVKtWLUvRSRQSM0Nv7NixAFxyySXZDkcyyH+f77zzzqDMf6e3bNkCQMKqecXMnTsXCLPJ999/H4AxY8ZkNE5lhiIi5GhmOHDgQCB1RnjttdcC4fhCqbqU7VcNgwYNAuD+++8PyvzaS3vLCM8444xge86cOUn7XnnlFQC++uorAOrUqZOROJUZioiQY5nhHXfcAcADDzyQVH799dcH276PUKqW5557rlhZnz59YohEKsr3EfqMMNV3tnbt2gAMGDAAgB49CpZYPvroowE4+OCDg2OvvPJKAMaPHw9Aw4YNATjggMw2X8oMRURQYygiAuTIZfI777wDwJ/+9Kekcn+z5OGHHw7K9ttP7XdVsmjRIgBefvnloMxfBnXr1i2WmKRi/OVs4g0TgGbNmgXbkydPBuC4444r9fOK3khr2rQpADVr1qxQnEWpZRERIUcywyFDhgCwadMmAH76058C4aQMygarrp07dyb9hLC+M/2XX7LjvvvuA8LhM61btwZg+vTpwTGNGjVK+d6vv/4agEmTJgVlfrC1v2J49tlnMxxxAbUyIiLkSGb40UcfJb320zkdeeSRcYQjWTR16tS4Q5CI+AHVPlNMlQ36yZr9I3aXXXYZAEuWLAmO8Rlm165dowsWZYYiIkDMmeG0adMA+OKLL4Bwip4LLrggtpgku/w0bVJ1HXbYYXvd5zPCkiZlPu+88wB4+umnMxtYEcoMRUSIOTMselfooosuAkqezqc0vg8CdBdaJA5169ZNeu0nXTjhhBOCsmOPPRaAZ555JunY6tWrA3DDDTcEZUOHDgWgRo0amQ82gVoLERFizgw3btyY9LpBgwZpf4ZfInTUqFEArFq1Ktg3ZcoUIPPTg0vF+XGFfiGvRM2bN892OJJBTzzxBACtWrUCYPv27QC89dZbwTF+sa+iV4GPPPIIkLxAWLYoMxQRQY2hiAgQw2Wyf+QOYPbs2Wm/36fcJ510EhBeZiU+zuX5udIyvVaCVJyvR3+5lKhTp07ZDkcywD82N2HCBCAcLF0Sf0z37t2BeC6PPWWGIiLEkBn6WXABtm3bVqb3TJw4Mdj2ayh/8sknpb7Pr7wluaekwdZ+kK3kruXLlwfbfiZqv1aJvylS9ObIySefHGyfddZZQDjd16uvvgrAzJkzAejcuXMEUZdMmaGICDFkhrVq1Qq2/WSPRbO8rVu3AuE0Ptdcc025zqUpoHLXXXfdlfQ68RHMNm3aZDscKSM/XO3yyy8Pynbs2JHy2Hbt2gHhBAu/+tWvgn1+uFuvXr2A8HG8G2+8EYDFixdnMuwyUWYoIkIMmaFfFQvCwbU+M/STua5btw6A/Pz8tD/fTyQJMGLEiPKGKRErOpKgXr16wfb++++f7XCkFDNmzADCjDAxG/SP3x1//PFAuO752WefDZS8/rX/vvoJnu+55x4AFixYEByT2NcYJWWGIiLE/DieX/DppZdeApL/GpSVv2Plxycl9kWVNHWQxGPt2rUA7Nq1K+ZIJB0ffPABEGaETZo0Cfb5O8B+oaZ0+PHB8+fPB8LRJomjTrJFmaGICGoMRUSAmC+Tu3TpAoSXs37G67Lo06cPAJdeeimg2bErCz9MavPmzUnlvh4lt/nH5/zco1C+y2M/fM5/jr/UjpMyQxERcmR1vKKuuOIKILztftVVVwX7tKZu5eTnmXzvvfeSyv2kDOeee27WY5Ky87NU+9mmR44cWeyYQYMGAcVnuv7yyy+B5Icr/JXAypUrgfBGaMuWLQE48cQTMxZ7WSkzFBEhxzJDP8utf2xHg2+rDj+QfvXq1Unlffv2BSq27o1Ez2fu999/PwD9+/cP9j344IMAPPnkk0C45ok3ffp0IHmgtu979PXuH917/PHHgXiu/JQZioiQI5mh1s7d95x++ukAdOvWLeZIJB0tWrQAktep8SMD/Pf4hRdeKPVz/Pt//vOfA/Db3/4WKPnRvagpMxQRIUcyQ6n6/LRcZZkKXnJXx44dgeQptvwjlrfffnvSsbNmzQKgUaNGAPTs2TPY5zPBXKLMUEQEZYYiUkE+8/N3gisrZYYiIqgxFBEB1BiKiABqDEVEADWGIiKAGkMREQAsnUGwZrYe+Cy6cHJOE+fcoXEHkU2q46pPdZxaWo2hiEhVpctkERHUGIqIAGoMRUSAiJ9NNrMGwOzCl4cD3wHrC1+f7JzbGdF56wN/BVoCDujrnEt/hXopVYx1PBY4H1jtnGsdxTmkQIx1/BvgCgq+wx8AVzrndpT8rgqcL1s3UMzsTmCbc+6BIuVWGMeeDJ5rPDDTOTfGzKoBNZ1zWzL1+ZJaluv4TOAbYLQaw+zJVh2bWRMKGuBWwA7gGeA559y4THx+KrFcJptZUzNbXNho/QdobGabE/b3NrO/Fm43MrNnzWyhmS0ws/alfHZ9oJ1zbgyAc26nGsLsi7KOAZxzc4CNkf0CUqqo6xg4EKhBwRVsLeB/EfwagTj7DJsDDznnWgKrSzjuEWCYc64t0IuCy1/MrJ2ZjUpx/DHAejP7u5ktMrPRZlYr08FLmURVx5I7Iqlj59xnwMPA58AaYJ1z7tVMB58ozvkMP3XOLSzDcZ2AZgmrp9Uzs5rOufnA/BTHHwC0BW4A3gP+BNwC/L7iIUuaoqpjyR2R1HFhP+UFwPeBrcBUM+vtnHs6Q3EXE2djuD1hew+QuFZkjYRtI71O2lXASl9BZjYV+HVFApVyi6qOJXdEVcc/AZY65zYAmNlzwClAZI1hTgytKex03WRmx5rZfkCPhN2zgH7+hZmV2FnunFsFrDWzpoVFHYHFJbxFsiCTdSy5KcN1vBLoYGY1C2/OdAQ+znTMiXKiMSx0KzADeIuC7M7rB5xqZh+a2WLgaii1P+kGYJKZfQj8CLgvurAlDRmrYzObAswFWprZKjP7v0gjl7LKSB07594EXgQWAR8Bu4EnogxczyaLiJBbmaGISGzUGIqIoMZQRARQYygiAqQ5zrBhw4YuLy8volByT35+Phs2bLDSj6w6VMdVn+o4tbQaw7y8PBYuLMtg86qhbdu2cYeQdarjqk91nJouk0VEUGMoIgKoMRQRAdQYiogAagxFRAA1hiIigBpDEREg3sldy2Xo0KEAPP10wRyP06ZNA+CYY46JLSYpv8WLC6aaHDFiRFD2+OOPA3DttdcCMGqUZv6X6CkzFBGhkmSGX375ZbDts4ZVqwrmjfzXv/4FKDOsbMaOHQvA4MGDgbA+Afw6Gf/4xz9SvnfcuHC1yJ/97GcA1KlTJ5I4Zd+hzFBEhEqSGfosApIzCKk8du3aBcCMGTMAuOaaa5LKy+LRRx8FoH///kHZ97//fQDuuusuAC655JKKBysZ8emnnwJhf/Cbb74Z7Pv444LlTHx/cN++fbMcXXHKDEVEqCSZ4euvvx53CFJBw4cPB2DgwIGlHtu8eXMAbrzxxqTyDRs2APDdd98FZcuWLQPguuuuSzpWGWL2+Sx/0qRJQJjtVatWDYBBgwYFx/pZc5QZiojkmJzODOfNmwck9zVI5eKzhQ8++KDE4xo3bhxsjx49GoDTTjutzOfZsmULEI5N9JnH/fffX/ZgpVx27ixYF96PDBg2bBgAP/rRjwB46KGHAOjcuXPwHt/3//nnnwMwd+5cAGrWrAnEM8+kMkMREdQYiogAOX6ZvHHjxqSfUjkk3uDwl6n+8cmizjjjDACmTp0alDVo0CDlsV27dgVgxYoVQdlTTz2VdM6tW7cC4SWaRGPHjh3B9i9/+UsgHAx/3HHHATBmzBgA2rRpU+z9Rx11FBAOlvf11aJFCwBmzpwZQdQlU2YoIkKOZ4YlOfzww4HwL4zkjnfffTfYvv3221Mec8oppwDw0ksvAWV7nM5nGE8++WRQNmfOHCA5W5To+IzwjjvuCMp8Rnj88ccD4cB6/x0tyZQpUwBYvXo1EA7D2b59e3BM7dq1Kxp2mSgzFBEhxzNDf0s+Ff9XqH379tkKR0rh++/uvvvuvR7jM8LZs2cDUL169egDk4zxmfwf//jHoOzoo48GYPr06UDZMkJv8+bNSa/r1q0LZC8bTKTMUESEHM8M/cSfqXTv3j2LkUhJ/AP5t912GxD2/yTyd419ZlGRjHDp0qXBdmLfEsAhhxwCaEq3TPPT6N1yyy0AHHTQQcE+/0jd9773vTJ/3po1awB45plnMhVihSkzFBEhxzPDkvgxZxK/Cy+8EEidEXp9+vQBMjMJa+IyAOvWrUvad+SRRwJhJiqZ4R93zM/PB+DEE08M9nXp0qXE9/oxoH7cIcA999wDwPLlyzMYZcUoMxQRIUczQ39X0v81SuT7Kvbff/+sxiTFTZ48GYAlS5YklSfeCezQoQOQmUz+iy++AMKJHFI54ogjKnweKZ2fYAHCsYJ+kgXvxRdfBML/TxK/z3l5eQDceuutQHh3Op070ZmmzFBEBDWGIiJAjl0m+wGYTzzxBJD8MLh30003AWFHucTHd6b7+ey8Vq1aBduvvPJKxs7nV0YsOpwGwqE6/rJLMsuvNeMfw/v9738f7OvVq1eJ7/VzVfp1aiCcmdxfbvvLZD8oPw7KDEVEyNHM0D987yUO0P3BD36Q1ZgkfX4t40xxzgHJU4MV5R/L7NixY0bPLQX8WtZ33nknAC1btgz2Pf/880nH+psgPmMs6ZFZPzi+devWQDiV294m+IiSMkMREXIsM/zmm29SlterVy/YzoVVtKRkp556akY/7+WXXwZg6NChez3mnHPOyeg5pWSJ/YSl9RmW5KuvvgLCCZz3NrFvNigzFBEhxzLDouvkeueee26WI5GKGDJkSLD92muvpf1+vz6yH1WQOJFoUb4P+Re/+EXa55H4rV+/HoCVK1cC0KNHj9hiUWYoIkKOZIb+r8OmTZuSyn0/0MiRI7Mek5Sfn54Jwskb9jYu1GcE48ePD8oeffRRIFxbtyQTJ04Ewse7pHJ5/fXXk143bNgwnkBQZigiAqgxFBEBcuQy2XeyL1y4MKm8Vq1aABxwQBjm7t27i5VJPPxNi0mTJgGwaNEiAP773/8Gx/iujvr166f8DD+D8rJly0o9X5MmTQDo3bt3UJb46J9UPrm0JroyQxERciQz3Jtp06YByfOkDR48GCh5AK5kh1/zwj86demllwLJE2wkrleSrgMPPBCAFi1aAGEG2qxZs3J/psjeKDMUESFHMkPfn+RXNis6w7XPEEBTd+UiP1D2pJNOApJXNSy6Lm5Z+EkA/GDriy++uKIhSiXh10OPgzJDERFyJDPs1KkTEA6u9ncp/bQ+N998c3DsZZddluXopKzmzZsHwP/+97+gbMKECUA4NdP8+fMBuPfee4HUa9n4TNDfPZZ9x7HHHhvbuZUZioiQI5mh57M+ZX+VW+IKdb/5zW+SforkKmWGIiLkWGYoIvumgw8+GIA6derEFoMyQxER1BiKiAC6TBaRGOXSDTZlhiIiqDEUEQHUGIqIAGDOubIfbLYe+Cy6cHJOE+fcoXEHkU2q46pPdZxaWo2hiEhVpctkERHUGIqIABE3hmbWwMzeL/z3hZmtTnhdLcLzjjWz9Wb2flTnkAIx1vH5ZvaJmS0zs1uiOo/EWscDzOw/hf9uiOo8wfmy1WdoZncC25xzDxQpt8I49mTwXGcC3wCjnXOtM/W5UrJs1bGZHQh8ApwNfAEsBC50zv23xDdKhWWxjlsDY4H2wG7gFeBK59yKTHx+KrFcJptZUzNbbGbjgf8Ajc1sc8L+3mb218LtRmb2rJktNLMFZta+tM93zs0BcmcNwn1QxHXcHvjYOfeZc24HMBn4WVS/i6QWcR23AN5xzn3jnNsFvAH0iOp3gXj7DJsDDznnWgKrSzjuEWCYc64t0Avw/3Hbmdmo6MOUCoiqjo8EPk94vaqwTLIvqjr+CDjTzOqbWW2gC9A4s6Eni/PZ5E+dcwtLP4xOQLOCLByAemZW0zk3H5gfWXSSCarjqi+SOnbO/dvMhgOzgG3AIuC7DMWcUpyN4faE7T2AJbyukbBtwMnOuZ1ZiUoyKao6Xk1ylnAUJWclEp3IvsfOudHAaAAzGwYsq0CcpcqJoTWFna6bzOxYM9uP5L6BWUA//6KwY1UqmQzX8TtASzNrYmbVKbjsejHTMUt6Mv09NrPDCn/mAd2ApzMZb1E50RgWuhWYAbxFQR+Q1w841cw+NLPFwNVQcp+hmU0B5lLwhVllZv8XaeRSVhmp48IO9f7ATGAxMM4590nUwUuZZOx7DDxfeOzzwHXOua0Rxq3H8UREILcyQxGR2KgxFBFBjaGICKDGUEQEUGMoIgKoMRQRAdQYiogAagxFRAD4f7+qqKRv+4foAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe5eea366d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n",
    "    \n",
    "# test function\n",
    "plot_images(train_data[:9], train_labels[:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network# Creat \n",
    "def conv_net(x_dict, n_classes, dropout, reuse, is_training):\n",
    "    \n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # TF Estimator input is a dict, in case of multiple inputs\n",
    "        x = x_dict['images']\n",
    "\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        # Convolution Layer with 32 filters and a kernel size of 5\n",
    "        conv1 = tf.layers.conv2d(x, 32, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 64 filters and a kernel size of 3\n",
    "        conv2 = tf.layers.conv2d(conv1, 64, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "        # Fully connected layer (in tf contrib folder for now)\n",
    "        fc1 = tf.layers.dense(fc1, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(fc1, n_classes)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model function (following TF Estimator Template)\n",
    "def model_fn(features, labels, mode):\n",
    "    \n",
    "    # Build the neural network\n",
    "    # Because Dropout have different behavior at training and prediction time, we\n",
    "    # need to create 2 distinct computation graphs that still share the same weights.\n",
    "    logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "    logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_classes = tf.argmax(logits_test, axis=1)\n",
    "    pred_probas = tf.nn.softmax(logits_test)\n",
    "    \n",
    "    # If prediction mode, early return\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=pred_classes) \n",
    "        \n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # Evaluate the accuracy of the model\n",
    "    acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "    \n",
    "    # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "    # the different ops for training, evaluating, ...\n",
    "    estim_specs = tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=pred_classes,\n",
    "      loss=loss_op,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "    return estim_specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_service': None, '_log_step_count_steps': 100, '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_model_dir': '/home/woodylin/tensorflow3/Udacity_self_driving_car/Udacity_Notebook/models/mnist', '_num_worker_replicas': 1, '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_is_chief': True, '_session_config': None, '_task_id': 0, '_task_type': 'worker', '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe5ec8475c0>, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "# Build the Estimator\n",
    "model = tf.estimator.Estimator(model_fn, model_dir=MODELS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /home/woodylin/tensorflow3/Udacity_self_driving_car/Udacity_Notebook/models/mnist/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.3263135, step = 1\n",
      "INFO:tensorflow:global_step/sec: 12.3774\n",
      "INFO:tensorflow:loss = 0.15098184, step = 101 (8.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.2569\n",
      "INFO:tensorflow:loss = 0.15142111, step = 201 (8.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3868\n",
      "INFO:tensorflow:loss = 0.04779297, step = 301 (8.073 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4178\n",
      "INFO:tensorflow:loss = 0.038057916, step = 401 (8.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4295\n",
      "INFO:tensorflow:loss = 0.027334906, step = 501 (8.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.357\n",
      "INFO:tensorflow:loss = 0.010292676, step = 601 (8.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4755\n",
      "INFO:tensorflow:loss = 0.024497136, step = 701 (8.016 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3957\n",
      "INFO:tensorflow:loss = 0.09965493, step = 801 (8.067 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5308\n",
      "INFO:tensorflow:loss = 0.058236577, step = 901 (7.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.5316\n",
      "INFO:tensorflow:loss = 0.012291832, step = 1001 (7.980 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4254\n",
      "INFO:tensorflow:loss = 0.01483768, step = 1101 (8.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5805\n",
      "INFO:tensorflow:loss = 0.0042889514, step = 1201 (8.635 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9413\n",
      "INFO:tensorflow:loss = 0.008371371, step = 1301 (9.140 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.605\n",
      "INFO:tensorflow:loss = 0.013507171, step = 1401 (9.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4834\n",
      "INFO:tensorflow:loss = 0.04836271, step = 1501 (9.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.26527\n",
      "INFO:tensorflow:loss = 0.08658218, step = 1601 (10.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.9906\n",
      "INFO:tensorflow:loss = 0.012314178, step = 1701 (8.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4869\n",
      "INFO:tensorflow:loss = 0.0065337527, step = 1801 (8.704 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6178\n",
      "INFO:tensorflow:loss = 0.003561504, step = 1901 (9.418 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /home/woodylin/tensorflow3/Udacity_self_driving_car/Udacity_Notebook/models/mnist/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.029499564.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x7fe5ec847470>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Parameters# Train \n",
    "learning_rate = 0.001\n",
    "num_steps = 2000\n",
    "batch_size = 128\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 784 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.25 # Dropout, probability to drop a unit\n",
    "\n",
    "# Define the input function for training\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={'images': train_data}, y=train_labels,\n",
    "    batch_size=batch_size, num_epochs=None, shuffle=True)\n",
    "# Train the Model\n",
    "model.train(input_fn, steps=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow3",
   "language": "python",
   "name": "tensorflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
