{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. Sequential Model\n",
    "\n",
    "The **keras.models.Sequential** class is a wrapper for the neural network model. It provides common functions like `fit()`, `evaluate()`, and `compile()`. We'll cover these functions as we get to them. Let's start looking at the layers of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras.models import Sequential\n",
    "\n",
    "#Create the Sequential model\n",
    "model = Sequential()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2-1. Layers\n",
    "\n",
    "A Keras layer is just like a neural network layer. There are fully connected layers, max pool layers, and activation layers. You can add a layer to the model using the model's `add()` function. For example, a simple model would look like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "\n",
    "#Create the Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "#1st Layer - Add a flatten layer\n",
    "model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "\n",
    "#2nd Layer - Add a fully connected layer\n",
    "model.add(Dense(100))\n",
    "\n",
    "#3rd Layer - Add a ReLU activation layer\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#4th Layer - Add a fully connected layer\n",
    "model.add(Dense(60))\n",
    "\n",
    "#5th Layer - Add a ReLU activation layer\n",
    "model.add(Activation('relu'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2-2. Or Another way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "#1st Layer - Add a flatten layer\n",
    "x = Flatten(input_shape=(32, 32, 3))\n",
    "\n",
    "#2nd Layer - Add a fully connected layer\n",
    "x = Dense(100)(x)\n",
    "\n",
    "#3rd Layer - Add a ReLU activation layer\n",
    "x = Activation('relu')(x)\n",
    "\n",
    "#4th Layer - Add a fully connected layer\n",
    "x = Dense(60)(x)\n",
    "\n",
    "#5th Layer - Add a ReLU activation layer\n",
    "x = Activation('relu')\n",
    "\n",
    "#Create the model\n",
    "model = Model(inputs, output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import Conv2D, Dense, Flatten, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(shape=(32,32,3), dtype='float32')\n",
    "x = Conv2D(32, [3,3], padding='valid', activation='relu')(input_data)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dense(5, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow3",
   "language": "python",
   "name": "tensorflow3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
